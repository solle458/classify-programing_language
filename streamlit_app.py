"""Programming Language Classifier - Streamlit Web App"""
import streamlit as st
import json
import sys
from pathlib import Path
import time

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.models.classifier import LogisticRegressionModel
from src.web.inference import WebInference, validate_file_extension, validate_file_size
from src.web.model_manager import ModelManager


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ¤– Programming Language Classifier",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)


@st.cache_resource
def load_model_info():
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
    with open("models_registry/model_info.json", "r") as f:
        return json.load(f)


@st.cache_resource
def load_model_and_preprocessor(model_id: str):
    """ãƒ¢ãƒ‡ãƒ«ã¨å‰å‡¦ç†å™¨ã‚’èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    try:
        model_manager = ModelManager()
        return model_manager.get_model_and_preprocessor(model_id)
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None, None


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.title("ğŸ¤– Programming Language Classifier")
    st.markdown("""
    ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ã€ã©ã®è¨€èªã®å¯èƒ½æ€§ãŒé«˜ã„ã‹ã‚’åˆ¤å®šã—ã¾ã™ã€‚
    100ç¨®é¡ä»¥ä¸Šã®ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    """)
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±èª­ã¿è¾¼ã¿
    try:
        model_info = load_model_info()
        active_models = [model for model in model_info["models"] if model["is_active"]]
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ¢ãƒ‡ãƒ«é¸æŠã¨æƒ…å ±
    with st.sidebar:
        st.header("ğŸ¤– ãƒ¢ãƒ‡ãƒ«é¸æŠ")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³
        selected_model_id = st.selectbox(
            "ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
            options=[model["id"] for model in active_models],
            format_func=lambda x: next(model["name"] for model in active_models if model["id"] == x),
            index=0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«
        )
        
        # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®è©³ç´°æƒ…å ±è¡¨ç¤º
        selected_model = next(model for model in active_models if model["id"] == selected_model_id)
        
        st.markdown("---")
        st.header("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©³ç´°")
        st.write(f"**åå‰**: {selected_model['name']}")
        st.write(f"**ã‚¿ã‚¤ãƒ—**: {selected_model['type']}")
        st.write(f"**ç²¾åº¦**: {selected_model['accuracy']:.4f}")
        st.write(f"**F1ã‚¹ã‚³ã‚¢**: {selected_model['f1_score']:.4f}")
        st.write(f"**ã‚µã‚¤ã‚º**: {selected_model['file_size_mb']:.1f} MB")
        
        with st.expander("ğŸ“ èª¬æ˜"):
            st.write(selected_model['description'])
        
        st.markdown("---")
        st.header("âš™ï¸ ãƒ¢ãƒ‡ãƒ«ç®¡ç†")
        
        # ãƒ¢ãƒ‡ãƒ«è¿½åŠ æ©Ÿèƒ½
        with st.expander("â• ãƒ¢ãƒ‡ãƒ«è¿½åŠ "):
            st.write("æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
            uploaded_model = st.file_uploader(
                "ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« (.joblib)",
                type=['joblib'],
                help="joblibå½¢å¼ã§ä¿å­˜ã•ã‚ŒãŸscikit-learnãƒ¢ãƒ‡ãƒ«"
            )
            
            if uploaded_model is not None:
                model_name = st.text_input("ãƒ¢ãƒ‡ãƒ«å", value=f"Custom Model {len(active_models)+1}")
                model_description = st.text_area("èª¬æ˜", value="ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«")
                
                if st.button("ğŸ“¤ ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ "):
                    add_custom_model(uploaded_model, model_name, model_description)
        
        # ãƒ¢ãƒ‡ãƒ«å‰Šé™¤æ©Ÿèƒ½
        if len(active_models) > 1:  # æœ€ä½1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã¯æ®‹ã™
            with st.expander("ğŸ—‘ï¸ ãƒ¢ãƒ‡ãƒ«å‰Šé™¤"):
                model_to_delete = st.selectbox(
                    "å‰Šé™¤ã™ã‚‹ãƒ¢ãƒ‡ãƒ«",
                    options=[model["id"] for model in active_models if model["id"] != selected_model_id],
                    format_func=lambda x: next(model["name"] for model in active_models if model["id"] == x),
                    help="ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã¯å‰Šé™¤ã§ãã¾ã›ã‚“"
                )
                
                if st.button("âŒ ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤", type="secondary"):
                    if st.session_state.get('confirm_delete', False):
                        delete_model(model_to_delete)
                        st.rerun()
                    else:
                        st.session_state.confirm_delete = True
                        st.warning("âš ï¸ æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹ï¼Ÿã‚‚ã†ä¸€åº¦ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„")
        
        st.markdown("---")
        st.markdown("**ğŸ“ å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼**")
        st.markdown(".py .js .java .cpp .c .h .cs .php .rb .go .rs .swift .kt .scala .r .sql .html .css .xml .json .yaml .md .txt ãªã©")
    
    # é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¨å‰å‡¦ç†å™¨èª­ã¿è¾¼ã¿
    with st.spinner(f"ğŸ”„ {selected_model['name']} ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        model, preprocessor = load_model_and_preprocessor(selected_model_id)
        if model is None or preprocessor is None:
            st.error("ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
    
    # æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
    inference_engine = WebInference(model, preprocessor)
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ï¼šæ¨è«–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
    st.header("ğŸ” ã‚³ãƒ¼ãƒ‰åˆ†æ")
    
    # ã‚¿ãƒ–ã§å…¥åŠ›æ–¹å¼ã‚’åˆ†ã‘ã‚‹
    tab1, tab2 = st.tabs(["ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "âœï¸ ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›"])
    
    with tab1:
        st.subheader("ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿")
        uploaded_file = st.file_uploader(
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
            type=None,  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«è¨±å¯ï¼ˆå¾Œã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            help="å¯¾å¿œå½¢å¼: .py, .js, .java, .cpp ãªã©ï¼ˆæœ€å¤§10MBï¼‰"
        )
        
        if uploaded_file is not None:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not validate_file_extension(uploaded_file.name):
                st.error("âŒ å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™")
                return
            
            if not validate_file_size(uploaded_file.size):
                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒ10MBã‚’è¶…ãˆã¦ã„ã¾ã™")
                return
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿å–ã‚Š
            try:
                content = uploaded_file.read().decode('utf-8')
                st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {uploaded_file.name}")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                with st.expander("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                    st.code(content[:1000] + ("..." if len(content) > 1000 else ""), language="text")
                
                # æ¨è«–å®Ÿè¡Œ
                if st.button("ğŸš€ è¨€èªã‚’åˆ¤å®š", key="file_predict"):
                    predict_and_display(inference_engine, content, uploaded_file.name)
                    
            except UnicodeDecodeError:
                st.error("âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ï¼ˆUTF-8ã®ã¿å¯¾å¿œï¼‰")
            except Exception as e:
                st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    with tab2:
        st.subheader("ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥å…¥åŠ›")
        text_input = st.text_area(
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            height=200,
            placeholder="ä¾‹:\ndef hello_world():\n    print('Hello, World!')"
        )
        
        if text_input.strip():
            if st.button("ğŸš€ è¨€èªã‚’åˆ¤å®š", key="text_predict"):
                predict_and_display(inference_engine, text_input, "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›")


def add_custom_model(uploaded_file, model_name: str, description: str):
    """ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ """
    import uuid
    import os
    
    try:
        # ä¸€æ„ã®IDã‚’ç”Ÿæˆ
        model_id = f"custom_{uuid.uuid4().hex[:8]}"
        model_filename = f"{model_id}.joblib"
        model_path = f"models_registry/{model_filename}"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
        file_size_mb = uploaded_file.size / (1024 * 1024)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        with open(model_path, "wb") as f:
            f.write(uploaded_file.read())
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’æ›´æ–°
        model_info = load_model_info()
        new_model = {
            "id": model_id,
            "name": model_name,
            "type": "custom",
            "file_path": model_path,
            "accuracy": 0.0,  # æœªçŸ¥
            "f1_score": 0.0,  # æœªçŸ¥
            "file_size_mb": round(file_size_mb, 2),
            "created_at": time.strftime("%Y-%m-%dT%H:%M:%S"),
            "is_active": True,
            "description": description
        }
        
        model_info["models"].append(new_model)
        
        # æ›´æ–°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜
        with open("models_registry/model_info.json", "w") as f:
            json.dump(model_info, f, indent=2)
        
        st.success(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model_name}' ãŒæ­£å¸¸ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸï¼")
        st.info("ãƒšãƒ¼ã‚¸ã‚’æ›´æ–°ã—ã¦ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        
    except Exception as e:
        st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")


def delete_model(model_id: str):
    """ãƒ¢ãƒ‡ãƒ«ã‚’å‰Šé™¤"""
    import os
    
    try:
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
        model_info = load_model_info()
        
        # å‰Šé™¤å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
        model_to_delete = next((model for model in model_info["models"] if model["id"] == model_id), None)
        if not model_to_delete:
            st.error("å‰Šé™¤å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if os.path.exists(model_to_delete["file_path"]):
            os.remove(model_to_delete["file_path"])
        
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‹ã‚‰å‰Šé™¤
        model_info["models"] = [model for model in model_info["models"] if model["id"] != model_id]
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãŒå‰Šé™¤ã•ã‚ŒãŸå ´åˆã€æ–°ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’è¨­å®š
        if model_info.get("default_model_id") == model_id:
            if model_info["models"]:
                model_info["default_model_id"] = model_info["models"][0]["id"]
        
        # æ›´æ–°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¿å­˜
        with open("models_registry/model_info.json", "w") as f:
            json.dump(model_info, f, indent=2)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        if 'confirm_delete' in st.session_state:
            del st.session_state.confirm_delete
        
        st.success(f"âœ… ãƒ¢ãƒ‡ãƒ« '{model_to_delete['name']}' ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ")
        
    except Exception as e:
        st.error(f"âŒ ãƒ¢ãƒ‡ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")


def predict_and_display(inference_engine: WebInference, code: str, source_name: str):
    """æ¨è«–å®Ÿè¡Œã¨çµæœè¡¨ç¤º"""
    
    with st.spinner("ğŸ¤– åˆ†æä¸­..."):
        result = inference_engine.predict_single_text(code)
    
    if not result["success"]:
        st.error(f"âŒ æ¨è«–ã‚¨ãƒ©ãƒ¼: {result['error']}")
        return
    
    # çµæœè¡¨ç¤ºã‚¨ãƒªã‚¢
    st.header("ğŸ“Š åˆ†æçµæœ")
    
    # åŸºæœ¬æƒ…å ±
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ğŸ¯ äºˆæ¸¬è¨€èª", result["predicted_language"])
    with col2:
        st.metric("â±ï¸ å‡¦ç†æ™‚é–“", f"{result['processing_time']:.3f}ç§’")
    
    # ä¸Šä½äºˆæ¸¬çµæœï¼ˆç¢ºç‡ä»˜ãï¼‰
    if "top_predictions" in result:
        st.subheader("ğŸ† ä¸Šä½äºˆæ¸¬çµæœ")
        for i, pred in enumerate(result["top_predictions"], 1):
            confidence_percent = pred["confidence"] * 100
            st.write(f"**{i}ä½**: {pred['language']} ({confidence_percent:.2f}%)")
            st.progress(pred["confidence"])
        
        # å…¨çµæœï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
        with st.expander("ğŸ“ˆ å…¨äºˆæ¸¬çµæœã‚’è¡¨ç¤º"):
            all_probs = result["all_probabilities"]
            sorted_probs = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)
            
            for lang, prob in sorted_probs[:20]:  # ä¸Šä½20ä½ã¾ã§è¡¨ç¤º
                st.write(f"{lang}: {prob*100:.2f}%")
    
    # åˆ†æå¯¾è±¡æƒ…å ±
    st.markdown("---")
    st.caption(f"ğŸ“ åˆ†æå¯¾è±¡: {source_name}")


if __name__ == "__main__":
    main()
