# æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¨å¥¨
_Exported on 2025/8/31 at 2:28:13 JST from Cursor (1.5.7)_

---

**User**

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¨€èªã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒãŠã™ã™ã‚ï¼Ÿ

---

**Cursor**

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¨€èªã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€é©åˆ‡ãªæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¨å¥¨ã™ã‚‹ãŸã‚ã«ã€ã¾ãšç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©³ç´°ã‚’ç¢ºèªã•ã›ã¦ãã ã•ã„ã€‚

ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è©³ã—ãè¦‹ã¦ã€ã•ã‚‰ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚‚ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

ã“ã‚Œã¾ã§ã®åˆ†æã‚’åŸºã«ã€ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‹ã‚‰è¨€èªã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ¨å¥¨äº‹é …ã‚’è©³ã—ãèª¬æ˜ã„ãŸã—ã¾ã™ã€‚

## ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰è¨€èªåˆ†é¡ã«ãŠã‘ã‚‹æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ¨å¥¨äº‹é …

ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆ†æçµæœã¨ä¸€èˆ¬çš„ãªãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’åŸºã«ã€ä»¥ä¸‹ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼š

### ğŸ¯ **ç¾åœ¨ã®å®Ÿè£…ï¼ˆæ¨å¥¨ç¶™ç¶šï¼‰**

**1. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° + TF-IDFï¼ˆç¾åœ¨ä½¿ç”¨ä¸­ï¼‰** âœ… **æœ€æ¨å¥¨**
- **ç²¾åº¦**: 78.71% (è»½é‡ç‰ˆ) / 80.84% (ãƒ•ãƒ«ç‰ˆ)
- **åˆ©ç‚¹**:
  - é«˜é€Ÿãªè¨“ç·´ãƒ»æ¨è«–
  - è»½é‡ï¼ˆ4.2MB vs 109MBï¼‰
  - è§£é‡ˆã—ã‚„ã™ã„
  - ãƒ‡ãƒ—ãƒ­ã‚¤ã«é©ã—ã¦ã„ã‚‹
  - 100+è¨€èªã«å¯¾å¿œ

### ğŸš€ **ä»–ã®æœ‰åŠ¹ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰**

**2. ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ**
```python
# src/models/classifier.py ã«å®Ÿè£…æ¸ˆã¿
model_type: "random_forest"
```
- åˆ©ç‚¹: éç·šå½¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚­ãƒ£ãƒƒãƒã€ç‰¹å¾´é‡ã®é‡è¦åº¦ãŒåˆ†ã‹ã‚‹
- æ¬ ç‚¹: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚ŠãŒã¡

**3. ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ãƒˆãƒ«ãƒã‚·ãƒ³ï¼ˆSVMï¼‰**
```python
# src/models/classifier.py ã«å®Ÿè£…æ¸ˆã¿  
model_type: "svm"
```
- åˆ©ç‚¹: é«˜æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã«å¼·ã„ã€ã‚«ãƒ¼ãƒãƒ«ã§éç·šå½¢å¯¾å¿œ
- æ¬ ç‚¹: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã¯é…ã„

### ğŸ“Š **å‰å‡¦ç†æ‰‹æ³•ã®æ¯”è¼ƒ**

| æ‰‹æ³• | åˆ©ç‚¹ | æ¬ ç‚¹ | ç”¨é€” |
|------|------|------|------|
| **TF-IDF**ï¼ˆç¾åœ¨ä½¿ç”¨ï¼‰| è¨€èªå›ºæœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•æŠ½å‡ºã€è»½é‡ | èªé †æƒ…å ±ãªã— | **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ»æœ¬ç•ªç’°å¢ƒ** |
| ã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– | ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜æ´»ç”¨ | æ‰‹å‹•ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å¿…è¦ | ç‰¹æ®Šè¦ä»¶æ™‚ |
| Word2Vec/FastText | æ„å‘³çš„é¡ä¼¼æ€§ | è¨“ç·´ã‚³ã‚¹ãƒˆé«˜ | é«˜ç²¾åº¦ãŒå¿…è¦ãªå ´åˆ |

### ğŸ¯ **æ¨å¥¨æ”¹å–„æ¡ˆ**

**æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:**

1. **ç¾åœ¨ã®TF-IDF + ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ç¶™ç¶š** (78-80%ç²¾åº¦ã§ååˆ†å®Ÿç”¨çš„)

2. **å¿…è¦ã«å¿œã˜ã¦ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã‚’æ¤œè¨:**
```python
# ä¾‹: æ§‹æ–‡çš„ç‰¹å¾´ + TF-IDF
features = combine_features([
    tfidf_features,      # ç¾åœ¨ã®æ‰‹æ³•
    syntax_features,     # ã‚«ã‚¹ã‚¿ãƒ æ§‹æ–‡ç‰¹å¾´
    token_features       # ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
])
```

3. **æ·±å±¤å­¦ç¿’ã‚’æ¤œè¨ã™ã‚‹å ´åˆ:**
   - BERTç³»ãƒ¢ãƒ‡ãƒ«ï¼ˆCodeBERTï¼‰
   - CNNï¼ˆæ–‡å­—ãƒ¬ãƒ™ãƒ«ï¼‰
   - LSTM/GRUï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚¹å­¦ç¿’ï¼‰

### ğŸ’¡ **æœ€çµ‚æ¨å¥¨äº‹é …**

**ç¾åœ¨ã® `ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° + TF-IDF` ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚**

**ç†ç”±:**
- âœ… é«˜ã„ç²¾åº¦ï¼ˆ78-80%ï¼‰
- âœ… è»½é‡ï¼ˆ4.2MBï¼‰
- âœ… é«˜é€Ÿæ¨è«–
- âœ… ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹æ€§è‰¯å¥½
- âœ… ãƒ‡ãƒ—ãƒ­ã‚¤å®¹æ˜“
- âœ… 100+è¨€èªå¯¾å¿œæ¸ˆã¿

### ğŸ”§ **è¨­å®šä¾‹**

ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§æœ€é©åŒ–æ¸ˆã¿ã®è¨­å®šï¼š

```yaml
# configs/lightweight.yamlï¼ˆæ¨å¥¨ï¼‰
data:
  lightweight: true  # è»½é‡åŒ–ãƒ¢ãƒ¼ãƒ‰
model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1000
    solver: "saga"
```

ã“ã®è¨­å®šã§ **78.71%ã®ç²¾åº¦** ã¨ **4.2MBã®è»½é‡æ€§** ã‚’ä¸¡ç«‹ã§ãã¦ãŠã‚Šã€å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã¨ã—ã¦ååˆ†ã§ã™ã€‚

ã‚ˆã‚Šé«˜ç²¾åº¦ãŒå¿…è¦ãªç‰¹æ®Šç”¨é€”ã§ãªã„é™ã‚Šã€ç¾åœ¨ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæœ€é©è§£ã¨åˆ¤æ–­ã•ã‚Œã¾ã™ã€‚

```python
# src/models/classifier.py ã«å®Ÿè£…æ¸ˆã¿
model_type: "random_forest"
```

```python
# src/models/classifier.py ã«å®Ÿè£…æ¸ˆã¿  
model_type: "svm"
```

```python
# ä¾‹: æ§‹æ–‡çš„ç‰¹å¾´ + TF-IDF
features = combine_features([
    tfidf_features,      # ç¾åœ¨ã®æ‰‹æ³•
    syntax_features,     # ã‚«ã‚¹ã‚¿ãƒ æ§‹æ–‡ç‰¹å¾´
    token_features       # ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³ã®å‡ºç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
])
```

```yaml
# configs/lightweight.yamlï¼ˆæ¨å¥¨ï¼‰
data:
  lightweight: true  # è»½é‡åŒ–ãƒ¢ãƒ¼ãƒ‰
model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1000
    solver: "saga"
```

---

**User**

ä»Šã¯è»½é‡åŒ–ã®é‡è¦åº¦ãŒé«˜ã‚ãªã®ã§ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ã—ã¦ãŠãã¾ã™
ã•ã‚‰ã«ç²¾åº¦ã‚’ä¸Šã’ã‚‹æ–¹æ³•ã¨ã—ã¦ä½•ãŒã‚ã’ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ

---

**Cursor**

è»½é‡åŒ–ã‚’é‡è¦–ã—ã¦ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ç¶™ç¶šã™ã‚‹ã¨ã„ã†åˆ¤æ–­ã¯éå¸¸ã«é©åˆ‡ã§ã™ï¼ç¾åœ¨ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç¶­æŒã—ãªãŒã‚‰ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’åˆ†æã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

è»½é‡åŒ–ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’è©³ã—ãåˆ†æã„ãŸã—ã¾ã™ã€‚ç¾åœ¨ã®è¨­å®šã‚’åŸºã«ã€å®Ÿè·µçš„ãªæ”¹å–„ç­–ã‚’ææ¡ˆã—ã¾ã™ã€‚

## ğŸ¯ è»½é‡ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®ç²¾åº¦å‘ä¸Šæˆ¦ç•¥

### ğŸ“Š **ç¾åœ¨ã®è¨­å®šåˆ†æ**

```python
# ç¾åœ¨ã®è»½é‡åŒ–è¨­å®š (78.71%ç²¾åº¦)
TfidfVectorizer(
    max_features=5000,      # ç‰¹å¾´é‡åˆ¶é™
    ngram_range=(1, 2),     # 1-gram, 2-gram
    max_df=0.95,            # 95%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹èªã‚’é™¤å¤–
    min_df=2,               # 2å›æœªæº€ã®èªã‚’é™¤å¤–
    stop_words='english'    # è‹±èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
)
```

### ğŸš€ **ç²¾åº¦å‘ä¸Šæ¡ˆï¼ˆè»½é‡æ€§ç¶­æŒï¼‰**

#### **1. TF-IDFãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–** ğŸ”§

```python
# ç¾åœ¨ã®è»½é‡åŒ–è¨­å®š (78.71%ç²¾åº¦)
TfidfVectorizer(
    max_features=5000,      # ç‰¹å¾´é‡åˆ¶é™
    ngram_range=(1, 2),     # 1-gram, 2-gram
    max_df=0.95,            # 95%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹èªã‚’é™¤å¤–
    min_df=2,               # 2å›æœªæº€ã®èªã‚’é™¤å¤–
    stop_words='english'    # è‹±èªã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å¤–
)
```

**A. ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç‰¹åŒ–è¨­å®š:**

```yaml
# æ”¹å–„æ¡ˆ1: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç‰¹åŒ–TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000â†’7500ï¼ˆ50%å¢—ã§è»½é‡æ€§ç¶­æŒï¼‰
    ngram_range=(1, 3),       # 3-gramã¾ã§æ‹¡å¼µï¼ˆè¨€èªæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    max_df=0.90,              # 0.95â†’0.90ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
    min_df=3,                 # 2â†’3ï¼ˆãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ï¼‰
    stop_words=None,          # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã«ã¯ä¸è¦
    analyzer='char_wb',       # æ–‡å­—ãƒ¬ãƒ™ãƒ«è§£æã‚‚è€ƒæ…®
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜å·ã‚‚ç‰¹å¾´ã«
)
```

**B. èªå½™ã®è³ªå‘ä¸Š:**

```python
# æ”¹å–„æ¡ˆ2: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å›ºæœ‰èªå½™ã®å¼·åŒ–
def custom_tokenizer(text):
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€æ¼”ç®—å­ã€åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä¿æŒ
    import re
    tokens = re.findall(r'\b\w+\b|[{}()[\];,.*+\-/=<>!&|]', text)
    return tokens

TfidfVectorizer(
    max_features=6000,
    ngram_range=(1, 2),
    tokenizer=custom_tokenizer,
    max_df=0.85,
    min_df=5
)
```

#### **2. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã®æœ€é©åŒ–** âš™ï¸

**A. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´:**

```yaml
# ç¾åœ¨ã®è¨­å®š
model:
  parameters:
    max_iter: 500      # å¢—åŠ å€™è£œ: 1000-2000
    solver: "saga"     # æœ€é©: L1/L2ä¸¡å¯¾å¿œ
    n_jobs: 1

# æ”¹å–„æ¡ˆ
model:
  parameters:
    max_iter: 1500           # åæŸæ€§å‘ä¸Š
    solver: "saga"           # ç¶­æŒ
    C: 10.0                  # æ­£å‰‡åŒ–å¼·åº¦ï¼ˆèª¿æ•´å€™è£œï¼‰
    l1_ratio: 0.15           # Elastic Netï¼ˆL1:L2=15:85ï¼‰
    penalty: "elasticnet"    # L1+L2ã§ç‰¹å¾´é¸æŠ+å®‰å®šæ€§
    class_weight: "balanced" # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
    random_state: 42
```

#### **3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆè»½é‡ï¼‰** ğŸ”¬

```yaml
# æ”¹å–„æ¡ˆ1: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç‰¹åŒ–TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000â†’7500ï¼ˆ50%å¢—ã§è»½é‡æ€§ç¶­æŒï¼‰
    ngram_range=(1, 3),       # 3-gramã¾ã§æ‹¡å¼µï¼ˆè¨€èªæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    max_df=0.90,              # 0.95â†’0.90ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
    min_df=3,                 # 2â†’3ï¼ˆãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ï¼‰
    stop_words=None,          # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã«ã¯ä¸è¦
    analyzer='char_wb',       # æ–‡å­—ãƒ¬ãƒ™ãƒ«è§£æã‚‚è€ƒæ…®
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜å·ã‚‚ç‰¹å¾´ã«
)
```

```yaml
# æ”¹å–„æ¡ˆ1: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç‰¹åŒ–TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000â†’7500ï¼ˆ50%å¢—ã§è»½é‡æ€§ç¶­æŒï¼‰
    ngram_range=(1, 3),       # 3-gramã¾ã§æ‹¡å¼µï¼ˆè¨€èªæ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    max_df=0.90,              # 0.95â†’0.90ï¼ˆã‚ˆã‚Šå³æ ¼ï¼‰
    min_df=3,                 # 2â†’3ï¼ˆãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ï¼‰
    stop_words=None,          # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã«ã¯ä¸è¦
    analyzer='char_wb',       # æ–‡å­—ãƒ¬ãƒ™ãƒ«è§£æã‚‚è€ƒæ…®
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨˜å·ã‚‚ç‰¹å¾´ã«
)
```

```python
# æ”¹å–„æ¡ˆ2: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å›ºæœ‰èªå½™ã®å¼·åŒ–
def custom_tokenizer(text):
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€æ¼”ç®—å­ã€åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ä¿æŒ
    import re
    tokens = re.findall(r'\b\w+\b|[{}()[\];,.*+\-/=<>!&|]', text)
    return tokens

TfidfVectorizer(
    max_features=6000,
    ngram_range=(1, 2),
    tokenizer=custom_tokenizer,
    max_df=0.85,
    min_df=5
)
```

```yaml
# ç¾åœ¨ã®è¨­å®š
model:
  parameters:
    max_iter: 500      # å¢—åŠ å€™è£œ: 1000-2000
    solver: "saga"     # æœ€é©: L1/L2ä¸¡å¯¾å¿œ
    n_jobs: 1

# æ”¹å–„æ¡ˆ
model:
  parameters:
    max_iter: 1500           # åæŸæ€§å‘ä¸Š
    solver: "saga"           # ç¶­æŒ
    C: 10.0                  # æ­£å‰‡åŒ–å¼·åº¦ï¼ˆèª¿æ•´å€™è£œï¼‰
    l1_ratio: 0.15           # Elastic Netï¼ˆL1:L2=15:85ï¼‰
    penalty: "elasticnet"    # L1+L2ã§ç‰¹å¾´é¸æŠ+å®‰å®šæ€§
    class_weight: "balanced" # ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡å¯¾å¿œ
    random_state: 42
```

**A. ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªå›ºæœ‰ç‰¹å¾´é‡:**

```python
def extract_programming_features(code_text):
    """è»½é‡ãªè¨€èªå›ºæœ‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    features = {}
    
    # 1. æ–‡å­—ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆè»½é‡ï¼‰
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4ã‚¹ãƒšãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
    
    # 2. æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè»½é‡ï¼‰
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# çµ„ã¿åˆã‚ã›
final_features = np.hstack([
    tfidf_features,           # ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ï¼ˆ5000æ¬¡å…ƒï¼‰
    programming_features      # è¿½åŠ ç‰¹å¾´é‡ï¼ˆ10-20æ¬¡å…ƒï¼‰
])  # åˆè¨ˆ: ~5020æ¬¡å…ƒï¼ˆè»½é‡æ€§ç¶­æŒï¼‰
```

#### **4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆè¶…è»½é‡ï¼‰** ğŸ¤

```python
def extract_programming_features(code_text):
    """è»½é‡ãªè¨€èªå›ºæœ‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    features = {}
    
    # 1. æ–‡å­—ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆè»½é‡ï¼‰
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4ã‚¹ãƒšãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
    
    # 2. æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè»½é‡ï¼‰
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# çµ„ã¿åˆã‚ã›
final_features = np.hstack([
    tfidf_features,           # ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ï¼ˆ5000æ¬¡å…ƒï¼‰
    programming_features      # è¿½åŠ ç‰¹å¾´é‡ï¼ˆ10-20æ¬¡å…ƒï¼‰
])  # åˆè¨ˆ: ~5020æ¬¡å…ƒï¼ˆè»½é‡æ€§ç¶­æŒï¼‰
```

```python
def extract_programming_features(code_text):
    """è»½é‡ãªè¨€èªå›ºæœ‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
    features = {}
    
    # 1. æ–‡å­—ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆè»½é‡ï¼‰
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4ã‚¹ãƒšãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
    
    # 2. æ§‹æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè»½é‡ï¼‰
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# çµ„ã¿åˆã‚ã›
final_features = np.hstack([
    tfidf_features,           # ãƒ¡ã‚¤ãƒ³ç‰¹å¾´é‡ï¼ˆ5000æ¬¡å…ƒï¼‰
    programming_features      # è¿½åŠ ç‰¹å¾´é‡ï¼ˆ10-20æ¬¡å…ƒï¼‰
])  # åˆè¨ˆ: ~5020æ¬¡å…ƒï¼ˆè»½é‡æ€§ç¶­æŒï¼‰
```

```python
# è»½é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: è¤‡æ•°ã®TF-IDFè¨­å®š
class LightweightEnsemble:
    def __init__(self):
        # 3ã¤ã®ç•°ãªã‚‹TF-IDFè¨­å®š
        self.models = [
            # Model 1: æ–‡å­—ãƒ¬ãƒ™ãƒ«
            (TfidfVectorizer(max_features=2000, analyzer='char', ngram_range=(2, 4)), 
             LogisticRegression(C=1.0)),
            
            # Model 2: å˜èªãƒ¬ãƒ™ãƒ«
            (TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 2)), 
             LogisticRegression(C=0.1)),
             
            # Model 3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
            (TfidfVectorizer(max_features=1000, analyzer='word', ngram_range=(1, 3)), 
             LogisticRegression(C=10.0))
        ]
    
    def predict(self, X):
        # è»½é‡æŠ•ç¥¨ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        predictions = []
        for vectorizer, model in self.models:
            X_transformed = vectorizer.transform(X)
            pred_proba = model.predict_proba(X_transformed)
            predictions.append(pred_proba)
        
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆç°¡å˜ãªé‡ã¿ï¼‰
        weights = [0.4, 0.4, 0.2]  # èª¿æ•´å¯èƒ½
        ensemble_proba = np.average(predictions, axis=0, weights=weights)
        return np.argmax(ensemble_proba, axis=1)

# åˆè¨ˆãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~6-8MBï¼ˆè»½é‡æ€§ç¶­æŒï¼‰
```

### ğŸ“ˆ **æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ**

| æ‰‹æ³• | ç²¾åº¦å‘ä¸Šäºˆæƒ³ | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | å®Ÿè£…é›£æ˜“åº¦ |
|------|------------|------------|------------|
| **TF-IDFæœ€é©åŒ–** | +2-3% | 4.2MBâ†’5.5MB | â­â­ |
| **æ­£å‰‡åŒ–èª¿æ•´** | +1-2% | 4.2MB | â­ |
| **ç‰¹å¾´é‡è¿½åŠ ** | +1-2% | 4.3MB | â­â­â­ |
| **è»½é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«** | +3-5% | 6-8MB | â­â­â­â­ |

### ğŸ¯ **æ¨å¥¨å®Ÿè£…é †åº**

```python
# è»½é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«: è¤‡æ•°ã®TF-IDFè¨­å®š
class LightweightEnsemble:
    def __init__(self):
        # 3ã¤ã®ç•°ãªã‚‹TF-IDFè¨­å®š
        self.models = [
            # Model 1: æ–‡å­—ãƒ¬ãƒ™ãƒ«
            (TfidfVectorizer(max_features=2000, analyzer='char', ngram_range=(2, 4)), 
             LogisticRegression(C=1.0)),
            
            # Model 2: å˜èªãƒ¬ãƒ™ãƒ«
            (TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 2)), 
             LogisticRegression(C=0.1)),
             
            # Model 3: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
            (TfidfVectorizer(max_features=1000, analyzer='word', ngram_range=(1, 3)), 
             LogisticRegression(C=10.0))
        ]
    
    def predict(self, X):
        # è»½é‡æŠ•ç¥¨ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
        predictions = []
        for vectorizer, model in self.models:
            X_transformed = vectorizer.transform(X)
            pred_proba = model.predict_proba(X_transformed)
            predictions.append(pred_proba)
        
        # é‡ã¿ä»˜ãå¹³å‡ï¼ˆç°¡å˜ãªé‡ã¿ï¼‰
        weights = [0.4, 0.4, 0.2]  # èª¿æ•´å¯èƒ½
        ensemble_proba = np.average(predictions, axis=0, weights=weights)
        return np.argmax(ensemble_proba, axis=1)

# åˆè¨ˆãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: ~6-8MBï¼ˆè»½é‡æ€§ç¶­æŒï¼‰
```

```plaintext

### ğŸ“ˆ **æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ**

| æ‰‹æ³• | ç²¾åº¦å‘ä¸Šäºˆæƒ³ | ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | å®Ÿè£…é›£æ˜“åº¦ |
|------|------------|------------|------------|
| **TF-IDFæœ€é©åŒ–** | +2-3% | 4.2MBâ†’5.5MB | â­â­ |
| **æ­£å‰‡åŒ–èª¿æ•´** | +1-2% | 4.2MB | â­ |
| **ç‰¹å¾´é‡è¿½åŠ ** | +1-2% | 4.3MB | â­â­â­ |
| **è»½é‡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«** | +3-5% | 6-8MB | â­â­â­â­ |

### ğŸ¯ **æ¨å¥¨å®Ÿè£…é †åº**
```

**1. å³åŠ¹æ€§é‡è¦–ï¼ˆæœ€åˆã«å®Ÿè£…ï¼‰**
```yaml
# Step 1: configs/optimized_lightweight.yaml
data:
  lightweight: true
  min_samples_per_class: 200

model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1500
    solver: "saga"
    C: 5.0                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‹ã‚‰èª¿æ•´
    penalty: "elasticnet"     # L1+L2
    l1_ratio: 0.15
    class_weight: "balanced"
```

**2. TF-IDFå¾®èª¿æ•´ï¼ˆä¸­ç¨‹åº¦åŠ¹æœï¼‰**
```python
# preprocessor.py ã®è»½é‡è¨­å®šã‚’ä¿®æ­£
TfidfVectorizer(
    max_features=6500,        # 5000â†’6500
    ngram_range=(1, 3),       # 3-gramã¾ã§
    max_df=0.90,              # ã‚ˆã‚Šå³æ ¼
    min_df=4,                 # ãƒã‚¤ã‚ºé™¤å»
    stop_words=None,          # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç”¨
    sublinear_tf=True         # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ”¹å–„
)
```

**3. ç‰¹å¾´é‡è¿½åŠ ï¼ˆé•·æœŸçš„æ”¹å–„ï¼‰**

### ğŸ’¡ **æœ€çµ‚æ¨å¥¨**

è»½é‡æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰ **82-85%ã®ç²¾åº¦** ã‚’ç›®æŒ‡ã™ãªã‚‰ï¼š

1. **å³å®Ÿè£…**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆC=5.0, elasticnetï¼‰
2. **æ¬¡æ®µéš**: TF-IDFæœ€é©åŒ–ï¼ˆmax_features=6500, 3-gramï¼‰
3. **å°†æ¥**: ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å›ºæœ‰ç‰¹å¾´é‡è¿½åŠ 

ã“ã®é †åºã§æ®µéšçš„ã«æ”¹å–„ã™ã‚Œã°ã€è»½é‡æ€§ï¼ˆ5-7MBä»¥ä¸‹ï¼‰ã‚’ä¿ã¡ãªãŒã‚‰ **3-7%ã®ç²¾åº¦å‘ä¸Š** ãŒæœŸå¾…ã§ãã¾ã™ã€‚

ã©ã®æ”¹å–„ç­–ã‹ã‚‰å§‹ã‚ã¦ã¿ãŸã„ã§ã—ã‚‡ã†ã‹ï¼Ÿå…·ä½“çš„ãªå®Ÿè£…ã‚’ãŠæ‰‹ä¼ã„ã§ãã¾ã™ï¼

```yaml
# Step 1: configs/optimized_lightweight.yaml
data:
  lightweight: true
  min_samples_per_class: 200

model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1500
    solver: "saga"
    C: 5.0                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‹ã‚‰èª¿æ•´
    penalty: "elasticnet"     # L1+L2
    l1_ratio: 0.15
    class_weight: "balanced"
```

```python
# preprocessor.py ã®è»½é‡è¨­å®šã‚’ä¿®æ­£
TfidfVectorizer(
    max_features=6500,        # 5000â†’6500
    ngram_range=(1, 3),       # 3-gramã¾ã§
    max_df=0.90,              # ã‚ˆã‚Šå³æ ¼
    min_df=4,                 # ãƒã‚¤ã‚ºé™¤å»
    stop_words=None,          # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªç”¨
    sublinear_tf=True         # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ”¹å–„
)
```

