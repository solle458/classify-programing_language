# 機械学習アルゴリズムの推奨
_Exported on 2025/8/31 at 2:28:13 JST from Cursor (1.5.7)_

---

**User**

ソースコードから言語を分類するタスクにおいて機械学習アルゴリズムがおすすめ？

---

**Cursor**

ソースコードから言語を分類するタスクにおいて、適切な機械学習アルゴリズムを推奨するために、まず現在のプロジェクトの詳細を確認させてください。

現在のプロジェクトを詳しく見て、さらに実装されているモデルの詳細も確認しましょう。

これまでの分析を基に、ソースコードから言語を分類するタスクにおける機械学習アルゴリズムの推奨事項を詳しく説明いたします。

## ソースコード言語分類における機械学習アルゴリズム推奨事項

現在のプロジェクトの分析結果と一般的なベストプラクティスを基に、以下をお勧めします：

### 🎯 **現在の実装（推奨継続）**

**1. ロジスティック回帰 + TF-IDF（現在使用中）** ✅ **最推奨**
- **精度**: 78.71% (軽量版) / 80.84% (フル版)
- **利点**:
  - 高速な訓練・推論
  - 軽量（4.2MB vs 109MB）
  - 解釈しやすい
  - デプロイに適している
  - 100+言語に対応

### 🚀 **他の有効なアルゴリズム（実装済み）**

**2. ランダムフォレスト**
```python
# src/models/classifier.py に実装済み
model_type: "random_forest"
```
- 利点: 非線形パターンをキャッチ、特徴量の重要度が分かる
- 欠点: モデルサイズが大きくなりがち

**3. サポートベクトルマシン（SVM）**
```python
# src/models/classifier.py に実装済み  
model_type: "svm"
```
- 利点: 高次元データに強い、カーネルで非線形対応
- 欠点: 大規模データでは遅い

### 📊 **前処理手法の比較**

| 手法 | 利点 | 欠点 | 用途 |
|------|------|------|------|
| **TF-IDF**（現在使用）| 言語固有パターン自動抽出、軽量 | 語順情報なし | **ベースライン・本番環境** |
| カスタムトークン化 | ドメイン知識活用 | 手動メンテナンス必要 | 特殊要件時 |
| Word2Vec/FastText | 意味的類似性 | 訓練コスト高 | 高精度が必要な場合 |

### 🎯 **推奨改善案**

**段階的アプローチ:**

1. **現在のTF-IDF + ロジスティック回帰を継続** (78-80%精度で十分実用的)

2. **必要に応じてハイブリッド手法を検討:**
```python
# 例: 構文的特徴 + TF-IDF
features = combine_features([
    tfidf_features,      # 現在の手法
    syntax_features,     # カスタム構文特徴
    token_features       # 特定トークンの出現パターン
])
```

3. **深層学習を検討する場合:**
   - BERT系モデル（CodeBERT）
   - CNN（文字レベル）
   - LSTM/GRU（シーケンス学習）

### 💡 **最終推奨事項**

**現在の `ロジスティック回帰 + TF-IDF` アプローチを継続することを強く推奨します。**

**理由:**
- ✅ 高い精度（78-80%）
- ✅ 軽量（4.2MB）
- ✅ 高速推論
- ✅ メンテナンス性良好
- ✅ デプロイ容易
- ✅ 100+言語対応済み

### 🔧 **設定例**

現在のプロジェクトで最適化済みの設定：

```yaml
# configs/lightweight.yaml（推奨）
data:
  lightweight: true  # 軽量化モード
model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1000
    solver: "saga"
```

この設定で **78.71%の精度** と **4.2MBの軽量性** を両立できており、実用レベルとして十分です。

より高精度が必要な特殊用途でない限り、現在のアプローチが最適解と判断されます。

```python
# src/models/classifier.py に実装済み
model_type: "random_forest"
```

```python
# src/models/classifier.py に実装済み  
model_type: "svm"
```

```python
# 例: 構文的特徴 + TF-IDF
features = combine_features([
    tfidf_features,      # 現在の手法
    syntax_features,     # カスタム構文特徴
    token_features       # 特定トークンの出現パターン
])
```

```yaml
# configs/lightweight.yaml（推奨）
data:
  lightweight: true  # 軽量化モード
model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1000
    solver: "saga"
```

---

**User**

今は軽量化の重要度が高めなのでロジスティックにしておきます
さらに精度を上げる方法として何があげられますか？

---

**Cursor**

軽量化を重視してロジスティック回帰を継続するという判断は非常に適切です！現在のアーキテクチャを維持しながら精度を向上させる方法を分析してみましょう。

軽量化を維持しながらロジスティック回帰の精度を向上させる方法を詳しく分析いたします。現在の設定を基に、実践的な改善策を提案します。

## 🎯 軽量ロジスティック回帰の精度向上戦略

### 📊 **現在の設定分析**

```python
# 現在の軽量化設定 (78.71%精度)
TfidfVectorizer(
    max_features=5000,      # 特徴量制限
    ngram_range=(1, 2),     # 1-gram, 2-gram
    max_df=0.95,            # 95%以上の文書に出現する語を除外
    min_df=2,               # 2回未満の語を除外
    stop_words='english'    # 英語ストップワード除外
)
```

### 🚀 **精度向上案（軽量性維持）**

#### **1. TF-IDFパラメータの最適化** 🔧

```python
# 現在の軽量化設定 (78.71%精度)
TfidfVectorizer(
    max_features=5000,      # 特徴量制限
    ngram_range=(1, 2),     # 1-gram, 2-gram
    max_df=0.95,            # 95%以上の文書に出現する語を除外
    min_df=2,               # 2回未満の語を除外
    stop_words='english'    # 英語ストップワード除外
)
```

**A. プログラミング言語特化設定:**

```yaml
# 改善案1: プログラミング言語特化TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000→7500（50%増で軽量性維持）
    ngram_range=(1, 3),       # 3-gramまで拡張（言語構文パターン）
    max_df=0.90,              # 0.95→0.90（より厳格）
    min_df=3,                 # 2→3（ノイズ除去強化）
    stop_words=None,          # プログラミング言語には不要
    analyzer='char_wb',       # 文字レベル解析も考慮
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # プログラミング記号も特徴に
)
```

**B. 語彙の質向上:**

```python
# 改善案2: プログラミング固有語彙の強化
def custom_tokenizer(text):
    # キーワード、演算子、区切り文字を保持
    import re
    tokens = re.findall(r'\b\w+\b|[{}()[\];,.*+\-/=<>!&|]', text)
    return tokens

TfidfVectorizer(
    max_features=6000,
    ngram_range=(1, 2),
    tokenizer=custom_tokenizer,
    max_df=0.85,
    min_df=5
)
```

#### **2. ロジスティック回帰の最適化** ⚙️

**A. ハイパーパラメータ調整:**

```yaml
# 現在の設定
model:
  parameters:
    max_iter: 500      # 増加候補: 1000-2000
    solver: "saga"     # 最適: L1/L2両対応
    n_jobs: 1

# 改善案
model:
  parameters:
    max_iter: 1500           # 収束性向上
    solver: "saga"           # 維持
    C: 10.0                  # 正則化強度（調整候補）
    l1_ratio: 0.15           # Elastic Net（L1:L2=15:85）
    penalty: "elasticnet"    # L1+L2で特徴選択+安定性
    class_weight: "balanced" # クラス不均衡対応
    random_state: 42
```

#### **3. 特徴量エンジニアリング（軽量）** 🔬

```yaml
# 改善案1: プログラミング言語特化TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000→7500（50%増で軽量性維持）
    ngram_range=(1, 3),       # 3-gramまで拡張（言語構文パターン）
    max_df=0.90,              # 0.95→0.90（より厳格）
    min_df=3,                 # 2→3（ノイズ除去強化）
    stop_words=None,          # プログラミング言語には不要
    analyzer='char_wb',       # 文字レベル解析も考慮
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # プログラミング記号も特徴に
)
```

```yaml
# 改善案1: プログラミング言語特化TF-IDF
TfidfVectorizer(
    max_features=7500,        # 5000→7500（50%増で軽量性維持）
    ngram_range=(1, 3),       # 3-gramまで拡張（言語構文パターン）
    max_df=0.90,              # 0.95→0.90（より厳格）
    min_df=3,                 # 2→3（ノイズ除去強化）
    stop_words=None,          # プログラミング言語には不要
    analyzer='char_wb',       # 文字レベル解析も考慮
    token_pattern=r'[a-zA-Z_]\w*|[{}()[\];,.]+'  # プログラミング記号も特徴に
)
```

```python
# 改善案2: プログラミング固有語彙の強化
def custom_tokenizer(text):
    # キーワード、演算子、区切り文字を保持
    import re
    tokens = re.findall(r'\b\w+\b|[{}()[\];,.*+\-/=<>!&|]', text)
    return tokens

TfidfVectorizer(
    max_features=6000,
    ngram_range=(1, 2),
    tokenizer=custom_tokenizer,
    max_df=0.85,
    min_df=5
)
```

```yaml
# 現在の設定
model:
  parameters:
    max_iter: 500      # 増加候補: 1000-2000
    solver: "saga"     # 最適: L1/L2両対応
    n_jobs: 1

# 改善案
model:
  parameters:
    max_iter: 1500           # 収束性向上
    solver: "saga"           # 維持
    C: 10.0                  # 正則化強度（調整候補）
    l1_ratio: 0.15           # Elastic Net（L1:L2=15:85）
    penalty: "elasticnet"    # L1+L2で特徴選択+安定性
    class_weight: "balanced" # クラス不均衡対応
    random_state: 42
```

**A. プログラミング言語固有特徴量:**

```python
def extract_programming_features(code_text):
    """軽量な言語固有特徴量を抽出"""
    features = {}
    
    # 1. 文字レベル統計（軽量）
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4スペースインデント
    
    # 2. 構文パターン（軽量）
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# 組み合わせ
final_features = np.hstack([
    tfidf_features,           # メイン特徴量（5000次元）
    programming_features      # 追加特徴量（10-20次元）
])  # 合計: ~5020次元（軽量性維持）
```

#### **4. アンサンブル（超軽量）** 🤝

```python
def extract_programming_features(code_text):
    """軽量な言語固有特徴量を抽出"""
    features = {}
    
    # 1. 文字レベル統計（軽量）
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4スペースインデント
    
    # 2. 構文パターン（軽量）
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# 組み合わせ
final_features = np.hstack([
    tfidf_features,           # メイン特徴量（5000次元）
    programming_features      # 追加特徴量（10-20次元）
])  # 合計: ~5020次元（軽量性維持）
```

```python
def extract_programming_features(code_text):
    """軽量な言語固有特徴量を抽出"""
    features = {}
    
    # 1. 文字レベル統計（軽量）
    features['avg_line_length'] = np.mean([len(line) for line in code_text.split('\n')])
    features['brace_ratio'] = code_text.count('{') + code_text.count('}')
    features['semicolon_ratio'] = code_text.count(';')
    features['indent_spaces'] = code_text.count('    ')  # 4スペースインデント
    
    # 2. 構文パターン（軽量）
    features['function_keywords'] = len(re.findall(r'\b(def|function|fun|func)\b', code_text))
    features['class_keywords'] = len(re.findall(r'\b(class|struct|interface)\b', code_text))
    features['import_statements'] = len(re.findall(r'\b(import|include|using|require)\b', code_text))
    
    return features

# 組み合わせ
final_features = np.hstack([
    tfidf_features,           # メイン特徴量（5000次元）
    programming_features      # 追加特徴量（10-20次元）
])  # 合計: ~5020次元（軽量性維持）
```

```python
# 軽量アンサンブル: 複数のTF-IDF設定
class LightweightEnsemble:
    def __init__(self):
        # 3つの異なるTF-IDF設定
        self.models = [
            # Model 1: 文字レベル
            (TfidfVectorizer(max_features=2000, analyzer='char', ngram_range=(2, 4)), 
             LogisticRegression(C=1.0)),
            
            # Model 2: 単語レベル
            (TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 2)), 
             LogisticRegression(C=0.1)),
             
            # Model 3: ハイブリッド
            (TfidfVectorizer(max_features=1000, analyzer='word', ngram_range=(1, 3)), 
             LogisticRegression(C=10.0))
        ]
    
    def predict(self, X):
        # 軽量投票（重み付き平均）
        predictions = []
        for vectorizer, model in self.models:
            X_transformed = vectorizer.transform(X)
            pred_proba = model.predict_proba(X_transformed)
            predictions.append(pred_proba)
        
        # 重み付き平均（簡単な重み）
        weights = [0.4, 0.4, 0.2]  # 調整可能
        ensemble_proba = np.average(predictions, axis=0, weights=weights)
        return np.argmax(ensemble_proba, axis=1)

# 合計モデルサイズ: ~6-8MB（軽量性維持）
```

### 📈 **期待される改善効果**

| 手法 | 精度向上予想 | モデルサイズ | 実装難易度 |
|------|------------|------------|------------|
| **TF-IDF最適化** | +2-3% | 4.2MB→5.5MB | ⭐⭐ |
| **正則化調整** | +1-2% | 4.2MB | ⭐ |
| **特徴量追加** | +1-2% | 4.3MB | ⭐⭐⭐ |
| **軽量アンサンブル** | +3-5% | 6-8MB | ⭐⭐⭐⭐ |

### 🎯 **推奨実装順序**

```python
# 軽量アンサンブル: 複数のTF-IDF設定
class LightweightEnsemble:
    def __init__(self):
        # 3つの異なるTF-IDF設定
        self.models = [
            # Model 1: 文字レベル
            (TfidfVectorizer(max_features=2000, analyzer='char', ngram_range=(2, 4)), 
             LogisticRegression(C=1.0)),
            
            # Model 2: 単語レベル
            (TfidfVectorizer(max_features=2000, analyzer='word', ngram_range=(1, 2)), 
             LogisticRegression(C=0.1)),
             
            # Model 3: ハイブリッド
            (TfidfVectorizer(max_features=1000, analyzer='word', ngram_range=(1, 3)), 
             LogisticRegression(C=10.0))
        ]
    
    def predict(self, X):
        # 軽量投票（重み付き平均）
        predictions = []
        for vectorizer, model in self.models:
            X_transformed = vectorizer.transform(X)
            pred_proba = model.predict_proba(X_transformed)
            predictions.append(pred_proba)
        
        # 重み付き平均（簡単な重み）
        weights = [0.4, 0.4, 0.2]  # 調整可能
        ensemble_proba = np.average(predictions, axis=0, weights=weights)
        return np.argmax(ensemble_proba, axis=1)

# 合計モデルサイズ: ~6-8MB（軽量性維持）
```

```plaintext

### 📈 **期待される改善効果**

| 手法 | 精度向上予想 | モデルサイズ | 実装難易度 |
|------|------------|------------|------------|
| **TF-IDF最適化** | +2-3% | 4.2MB→5.5MB | ⭐⭐ |
| **正則化調整** | +1-2% | 4.2MB | ⭐ |
| **特徴量追加** | +1-2% | 4.3MB | ⭐⭐⭐ |
| **軽量アンサンブル** | +3-5% | 6-8MB | ⭐⭐⭐⭐ |

### 🎯 **推奨実装順序**
```

**1. 即効性重視（最初に実装）**
```yaml
# Step 1: configs/optimized_lightweight.yaml
data:
  lightweight: true
  min_samples_per_class: 200

model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1500
    solver: "saga"
    C: 5.0                    # デフォルトから調整
    penalty: "elasticnet"     # L1+L2
    l1_ratio: 0.15
    class_weight: "balanced"
```

**2. TF-IDF微調整（中程度効果）**
```python
# preprocessor.py の軽量設定を修正
TfidfVectorizer(
    max_features=6500,        # 5000→6500
    ngram_range=(1, 3),       # 3-gramまで
    max_df=0.90,              # より厳格
    min_df=4,                 # ノイズ除去
    stop_words=None,          # プログラミング言語用
    sublinear_tf=True         # スケーリング改善
)
```

**3. 特徴量追加（長期的改善）**

### 💡 **最終推奨**

軽量性を維持しながら **82-85%の精度** を目指すなら：

1. **即実装**: ハイパーパラメータ調整（C=5.0, elasticnet）
2. **次段階**: TF-IDF最適化（max_features=6500, 3-gram）
3. **将来**: プログラミング固有特徴量追加

この順序で段階的に改善すれば、軽量性（5-7MB以下）を保ちながら **3-7%の精度向上** が期待できます。

どの改善策から始めてみたいでしょうか？具体的な実装をお手伝いできます！

```yaml
# Step 1: configs/optimized_lightweight.yaml
data:
  lightweight: true
  min_samples_per_class: 200

model:
  model_type: "logistic_regression"
  parameters:
    max_iter: 1500
    solver: "saga"
    C: 5.0                    # デフォルトから調整
    penalty: "elasticnet"     # L1+L2
    l1_ratio: 0.15
    class_weight: "balanced"
```

```python
# preprocessor.py の軽量設定を修正
TfidfVectorizer(
    max_features=6500,        # 5000→6500
    ngram_range=(1, 3),       # 3-gramまで
    max_df=0.90,              # より厳格
    min_df=4,                 # ノイズ除去
    stop_words=None,          # プログラミング言語用
    sublinear_tf=True         # スケーリング改善
)
```

